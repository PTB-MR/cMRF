{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-source cardiac MR Fingerprinting\n",
    "This notebook provides the image reconstruction and parameter estimation methods required to reproduce the multi-scanner comparison carried out in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook the cardiac MR Fingerprinting (cMRF) data acquired at four different scanners and the corresponding spin-echo reference sequences are reconstructed and $T_1$ and $T_2$ maps are estimated. Average $T_1$ and $T_2$ are calculated in circular ROIs for different tissue types represented in the phantom. \n",
    "\n",
    "This notebook utilises MRpro for reconstruction and parameter estimation: https://github.com/PTB-MR/mrpro and it is designed to be run via Google colab: \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PTB-MR/cMRF/open_source_cmrf_scanner_comparison.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "It will take roughly 1h to run the notebook for all four datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1) Installation of MRpro\n",
    "\n",
    "2) Download data from zenodo\n",
    "\n",
    "3) Define image reconstruction and parameter estimation methods for cMRF and reference sequences\n",
    "\n",
    "4) Define evaluation methods\n",
    "\n",
    "5) Run through all datasets and calculate $T_1$ and $T_2$ maps\n",
    "\n",
    "6) Visualise and evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Installation of MRpro\n",
    "Install MRpro (currently a working github-branch but will be changed to pypi package as soon as possible) and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/PTB-MR/mrpro.git@epg#egg=mrpro[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "import zenodo_get\n",
    "from matplotlib.colors import ListedColormap\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "from mrpro.utils import split_idx\n",
    "from mrpro.data import KData, DcfData, IData, CsmData \n",
    "from mrpro.data.traj_calculators import KTrajectoryIsmrmrd\n",
    "from mrpro.algorithms.reconstruction import DirectReconstruction\n",
    "from mrpro.operators.models.EPG import EpgMrfFispWithPreparation\n",
    "from mrpro.data.traj_calculators import KTrajectoryCartesian\n",
    "from mrpro.operators.models import MonoExponentialDecay, InversionRecovery\n",
    "from mrpro.operators import MagnitudeOp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Download data from zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data, ROIs, the flip angle pattern and the colormaps\n",
    "data_folder = Path(tempfile.mkdtemp())\n",
    "dataset = '14251660'\n",
    "zenodo_get.zenodo_get([dataset, '-r', 5, '-o', data_folder])  # r: retries\n",
    "with zipfile.ZipFile(data_folder / Path('open_source_cmrf_scanner_comparison.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Define image reconstruction and parameter estimation methods for cMRF and reference sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all scans we carry out dictionary matching to estimate the quantitative parameters from a series of qualitative images. So let's start by defining a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the dictionary matching\n",
    "def dictionary_matching(img_data, model, dictionary_values):\n",
    "    dictionary_values = dictionary_values.to(dtype=torch.float32)\n",
    "    (signal_dictionary,) = model(torch.ones(1), dictionary_values)\n",
    "    signal_dictionary = signal_dictionary.to(dtype=torch.complex64)\n",
    "    vector_norm = torch.linalg.vector_norm(signal_dictionary, dim=0)\n",
    "    signal_dictionary /= vector_norm\n",
    "    signal_dictionary = signal_dictionary.to(img_data.dtype)\n",
    "\n",
    "    # Calculate the dot-product \n",
    "    # and select for each voxel the values that correspond to the maximum of the dot-product\n",
    "    n_y, n_x = img_data.shape[-2:]\n",
    "    dot_product = torch.mm(rearrange(img_data, 'other 1 z y x->(z y x) other'), signal_dictionary)\n",
    "    idx_best_match = torch.argmax(torch.abs(dot_product), dim=1)\n",
    "    return rearrange(dictionary_values[idx_best_match], '(y x)->1 1 y x', y=n_y, x=n_x)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the reconstruction and parameter estimation for the reference sequences. We use a DirectReconstruction which in this case carries out a Fast Fourier Transform. The data from the different receiver coils were combined by a weighted sum using coil sensitivity maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to reconstruct the reference scans\n",
    "def multi_image_reco(kdata):\n",
    "    reco = DirectReconstruction(kdata=kdata, csm=None)\n",
    "    img = reco(kdata)\n",
    "    first_image = IData(data=img.data[0,None], header=img.header)\n",
    "    csm_first_image = CsmData.from_idata_inati(first_image)\n",
    "    reco = DirectReconstruction(kdata=kdata, csm=csm_first_image)\n",
    "    return reco(kdata)\n",
    "\n",
    "# Dictionary matching using a mono-exponential model for T2\n",
    "def dictionary_matching_spin_echo_t2(img, t2):\n",
    "    model = MonoExponentialDecay(decay_time=img.header.te*1000)\n",
    "    return dictionary_matching(img.data, model, dictionary_values=t2)\n",
    "    \n",
    "# Dictionary matching using a inversion recovery model for T1    \n",
    "def dictionary_matching_spin_echo_t1(img, t1):\n",
    "    model = MagnitudeOp() @ InversionRecovery(ti=img.header.ti*1000)\n",
    "    return dictionary_matching(img.data.abs(), model, dictionary_values=t1)\n",
    "\n",
    "# Reconstruct T1 and T2 spin-echo reference scans and return T1 and T2 maps\n",
    "def reco_ref_scans(pname_ref_t1, fname_t1_se_pulseq, pname_ref_t2, fname_t2_se_pulseq, t1, t2):\n",
    "    # Multi echo SE pulseq\n",
    "    kdata = KData.from_file( pname_ref_t2 / fname_t2_se_pulseq, KTrajectoryCartesian())\n",
    "    kdata.header.recon_matrix.y = 128\n",
    "    img_multi_echo_se_pulseq = multi_image_reco(kdata)\n",
    "    t2_map_se_pulseq = dictionary_matching_spin_echo_t2(img_multi_echo_se_pulseq, t2)[0,0,...]\n",
    "        \n",
    "    # Multi TI SE pulseq\n",
    "    kdata = KData.from_file(pname_ref_t1 / fname_t1_se_pulseq, KTrajectoryCartesian())\n",
    "    kdata.header.recon_matrix.y = 128\n",
    "    kdata.header.ti = torch.as_tensor([25, 50, 300, 600, 1200, 2400, 4800])/1000\n",
    "    img_multi_ti_se_pulseq = multi_image_reco(kdata)\n",
    "    t1_map_se_pulseq = dictionary_matching_spin_echo_t1(img_multi_ti_se_pulseq, t1)[0,0,...]\n",
    "    \n",
    "    return t1_map_se_pulseq, t2_map_se_pulseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the function to reconstruct the cMRF data and estimate the $T_1$ and $T_2$ maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to reconstruct the cMRF scans\n",
    "def reco_cMRF_scans(pname, scan_name, fa, t1, t2):\n",
    "    n_lines_per_img = 20\n",
    "    n_lines_overlap= 10\n",
    "\n",
    "    # Image reconstruction of average image\n",
    "    kdata = KData.from_file(pname / scan_name, KTrajectoryIsmrmrd())\n",
    "    avg_recon = DirectReconstruction(kdata)\n",
    "    \n",
    "    # Split data into dynamics and reconstruct\n",
    "    dyn_idx = split_idx(torch.arange(0,47), n_lines_per_img, n_lines_overlap)\n",
    "    dyn_idx = torch.cat([dyn_idx + ind*47 for ind in range(15)], dim=0)\n",
    "    \n",
    "    kdata_dyn = kdata.split_k1_into_other(dyn_idx, other_label='repetition')\n",
    "    \n",
    "    dyn_recon = DirectReconstruction(kdata_dyn, csm=avg_recon.csm)\n",
    "    dcf_data_dyn = rearrange(avg_recon.dcf.data, 'k2 k1 other k0->other k2 k1 k0')\n",
    "    dcf_data_dyn = rearrange(dcf_data_dyn[dyn_idx.flatten(),...], '(other k1) 1 k2 k0->other k2 k1 k0', k1=dyn_idx.shape[-1])\n",
    "    dyn_recon.dcf = DcfData(dcf_data_dyn)\n",
    "\n",
    "    img = dyn_recon(kdata_dyn).rss()[:,0,:,:]\n",
    "\n",
    "    # Dictionary settings\n",
    "    t1, t2 = torch.broadcast_tensors(t1[None,:], t2[:,None])\n",
    "    t1_all = t1.flatten().to(dtype=torch.float32)\n",
    "    t2_all = t2.flatten().to(dtype=torch.float32)\n",
    "    \n",
    "    t1 = t1_all[t1_all >= t2_all]\n",
    "    t2 = t2_all[t1_all >= t2_all]\n",
    "    m0 = torch.ones_like(t1)\n",
    "\n",
    "    # Dictionary calculation\n",
    "    n_rf_pulses_per_block = 47 # 47 RF pulses in each block\n",
    "    acq_t_ms = kdata.header.acq_info.acquisition_time_stamp[0,0,:,0]*2.5\n",
    "    delay_between_blocks = [acq_t_ms[n_block*n_rf_pulses_per_block] - acq_t_ms[n_block*n_rf_pulses_per_block-1] for n_block in range(1,3*5)]\n",
    "    delay_between_blocks.append(delay_between_blocks[-1]) # last delay is not needed but makes computations easier\n",
    "\n",
    "    flip_angles = fa\n",
    "    rf_phases = 0.0\n",
    "    te = 1.52\n",
    "    tr = kdata.header.tr * 1000\n",
    "    inv_prep_ti = [21,None,None,None,None]*3 # 21 ms delay after inversion pulse in block 0\n",
    "    t2_prep_te = [None,None,30,50,100]*3 # T2-preparation pulse with TE = 30, 50, 100\n",
    "    delay_due_to_prep = [0, 30, 50, 100, 21]*3\n",
    "    delay_after_block = [trig_delay-prep_delay for prep_delay, trig_delay in zip(delay_due_to_prep, delay_between_blocks)]\n",
    "    epg_mrf_fisp = EpgMrfFispWithPreparation(flip_angles, rf_phases, te, tr, inv_prep_ti, t2_prep_te, n_rf_pulses_per_block, delay_after_block)\n",
    "    (signal_dictionary,) = epg_mrf_fisp.forward(m0, t1, t2)\n",
    "\n",
    "\n",
    "    signal_dictionary = rearrange(signal_dictionary[dyn_idx.flatten(),...], '(other k1) t->other t k1', k1=dyn_idx.shape[-1])\n",
    "    signal_dictionary = torch.mean(signal_dictionary, dim=-1)\n",
    "    signal_dictionary = signal_dictionary.abs()\n",
    "\n",
    "    # Normalise dictionary entries\n",
    "    vector_norm = torch.linalg.vector_norm(signal_dictionary, dim=0)\n",
    "    signal_dictionary /= vector_norm\n",
    "\n",
    "    # Dictionary matching\n",
    "    n_y, n_x = img.shape[-2:]\n",
    "    dot_product = torch.mm(rearrange(img.abs(), 'other y x->(y x) other'), signal_dictionary)\n",
    "    idx_best_match = torch.argmax(torch.abs(dot_product), dim=1)\n",
    "    t1_match = rearrange(t1[idx_best_match], '(y x)->y x', y=n_y, x=n_x)\n",
    "    t2_match = rearrange(t2[idx_best_match], '(y x)->y x', y=n_y, x=n_x)\n",
    "    \n",
    "    return t1_match, t2_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Define evaluation methods\n",
    "\n",
    "The phantom we used for data acquisition consists of nine tubes, each representing a different cardiac tissue type. Now we define a method to calculate the mean value and standard deviation over a circular ROI in each of these tubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the mean and standard deviation of the tubes in the image data\n",
    "def image_statistics(idat, mask_name):\n",
    "    \n",
    "    if mask_name is not None:\n",
    "        mask = np.squeeze(np.load(mask_name))\n",
    "        \n",
    "    number_of_tubes = 9\n",
    "    mean = []\n",
    "    std_deviation = []\n",
    "    for idx_value in range(number_of_tubes):                      \n",
    "        mean.append(torch.mean(idat[mask==idx_value+1]))\n",
    "        std_deviation.append(torch.std(idat[mask==idx_value+1]))\n",
    "\n",
    "    return mean, std_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Run through all datasets and calculate $T_1$ and $T_2$ maps\n",
    "\n",
    "Now we can go through the acquisitions at the different scanners, reconstruct the cMRF and reference scans, estimate $T_1$ and $T_2$ maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the T1 and T2 values to be included in the dictionaries\n",
    "t1 = torch.cat((torch.arange(50, 2000+10, 10), torch.arange(2020, 3000+20, 20), torch.arange(3050,5000+50,50)))\n",
    "t2 = torch.cat((torch.arange(6, 100+2, 2), torch.arange(105, 200+5, 5), torch.arange(220,500+20,20)))\n",
    "    \n",
    "# Read in flip angle pattern\n",
    "fname_angle = data_folder / Path('cMRF_fa_705rep.txt')\n",
    "\n",
    "with open(fname_angle, \"r\") as file:\n",
    "    fa = torch.as_tensor([float(line) for line in file.readlines()])/180 * torch.pi\n",
    "\n",
    "cmrf_t1_maps = []\n",
    "cmrf_t2_maps = []\n",
    "ref_t1_maps = []\n",
    "ref_t2_maps = []\n",
    "\n",
    "for scanner_idx in range(4):\n",
    "    # Current path of data\n",
    "    pname = data_folder / Path(f'scanner{scanner_idx+1}/')\n",
    "    \n",
    "    # Reference T1 and T2 maps\n",
    "    t1_map_ref, t2_map_ref =  reco_ref_scans(pname, 'ref_t1.h5', pname, 'ref_t2.h5', t1, t2)  \n",
    "    ref_t1_maps.append(t1_map_ref)\n",
    "    ref_t2_maps.append(t2_map_ref)\n",
    "    \n",
    "    # cMRF T1 and T2 maps\n",
    "    t1_map_cmrf, t2_map_cmrf = reco_cMRF_scans(pname, 'cMRF.h5', fa, t1, t2)\n",
    "    cmrf_t1_maps.append(t1_map_cmrf)\n",
    "    cmrf_t2_maps.append(t2_map_cmrf)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Visualise and evaluate results\n",
    "\n",
    "Now we visualise and compare all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recommended colormaps\n",
    "cmap_t1 = ListedColormap(np.loadtxt(data_folder / Path('lipari.csv')))\n",
    "cmap_t2 = ListedColormap(np.loadtxt(data_folder / Path('navia.csv')))\n",
    "\n",
    "# Plot T1 and T2 maps\n",
    "for scanner_idx in range(4):\n",
    "    fig, ax = plt.subplots(2,2)\n",
    "    for cax in ax.flatten():\n",
    "        cax.set_axis_off()\n",
    "\n",
    "    im = ax[0,0].imshow(cmrf_t1_maps[scanner_idx], vmin=0, vmax=2000, cmap=cmap_t1)\n",
    "    ax[0,0].set_title('cMRF T1 (ms)')\n",
    "    plt.colorbar(im)\n",
    "    im = ax[1,0].imshow(cmrf_t2_maps[scanner_idx], vmin=0, vmax=200, cmap=cmap_t2)\n",
    "    ax[1,0].set_title('cMRF T2 (ms)')\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "    ax[0,1].imshow(ref_t1_maps[scanner_idx], vmin=0, vmax=2000, cmap=cmap_t1)\n",
    "    ax[0,1].set_title('Reference T1 (ms)')\n",
    "    ax[1,1].imshow(ref_t2_maps[scanner_idx], vmin=0, vmax=200, cmap=cmap_t2)\n",
    "    ax[1,1].set_title('Reference T1 (ms)')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ROI values\n",
    "fig, ax = plt.subplots(2,4, figsize=(12,7))\n",
    "for scanner_idx in range(4):\n",
    "    # Current path of data\n",
    "    pname = data_folder / Path(f'scanner{scanner_idx+1}/')\n",
    "    \n",
    "    t1_mean_cmrf, t1_std_cmrf = image_statistics(cmrf_t1_maps[scanner_idx], pname / \"mask.npy\") \n",
    "    t2_mean_cmrf, t2_std_cmrf = image_statistics(cmrf_t2_maps[scanner_idx], pname / \"mask.npy\") \n",
    "    t1_mean_ref, t1_std_ref = image_statistics(ref_t1_maps[scanner_idx], pname / \"mask.npy\") \n",
    "    t2_mean_ref, t2_std_ref = image_statistics(ref_t2_maps[scanner_idx], pname / \"mask.npy\") \n",
    "    \n",
    "    ax[0,scanner_idx].set_title(f'Scanner {scanner_idx+1}')\n",
    "    \n",
    "    # Plot T1 data\n",
    "    ax[0,scanner_idx].errorbar(t1_mean_ref, t1_mean_cmrf, t1_std_cmrf, t1_std_ref, fmt=\"o\", color=\"teal\")\n",
    "    ax[0,scanner_idx].plot([0, 2000], [0, 2000], color=\"darkorange\")\n",
    "    \n",
    "    # Plot T2 data\n",
    "    ax[1,scanner_idx].errorbar(t2_mean_ref, t2_mean_cmrf, t2_std_cmrf, t2_std_ref, fmt=\"o\", color=\"teal\")\n",
    "    ax[1,scanner_idx].plot([0, 200], [0, 200], color=\"darkorange\")\n",
    "    \n",
    "    for pidx in range(2):\n",
    "        ax[pidx,scanner_idx].set_xlabel(f'T{int(pidx+1)} - Reference (ms)',fontsize=12)\n",
    "        ax[pidx,scanner_idx].set_ylabel(f'T{int(pidx+1)} - cMRF (ms)',fontsize=12)\n",
    "        ax[pidx,scanner_idx].grid()\n",
    "        ax[pidx,scanner_idx].set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
